## PERSONALIZED EMOJIS – A DEEP LEARNING METHOD FOR CREATING AND RECOGNISING UNIQUE EMOJIS
  The Integration of ML system, aimed at producing suitable emojis for facial expressions .
## About
 Facial emoji generation is a human-computer interaction system. Face expressions are a key feature of non-verbal communication, and they play an important role in Human Computer Interaction. Emoji Generation in real time by recognizing the facial expression of a person has always been challenging. Facial expressions are vital to social communication between humans. As the world is emerging with new technology every day, there are more virtual interactions like text messages than the real ones. Emoticons help in social interaction virtually, with less exchange of words. This paper presents an approach of Emoji Generation using Facial Expression Recognition(FER) using Convolutional Neural Networks(CNN) with Machine Learning and Deep learning. This model created using CNN can be used to detect facial expressions in real time. The system can be used for analysis of emotions while users watch movie trailers or video lectures, feedback processing.

## Features
<!--List the features of the project as shown below-->
- Implements advance neural network method.
- A framework based application for deployment purpose.
- High scalability.
- Less time complexity.
- A specific scope of Chatbot response model, using json data format.

## Requirements
<!--List the requirements of the project as shown below-->
* Operating System: Requires a 64-bit OS (Windows 10 or Ubuntu) for compatibility with deep learning frameworks.
* Development Environment: Python 3.6 or later is necessary for coding the sign language detection system.
* Deep Learning Frameworks: TensorFlow for model training, MediaPipe for hand gesture recognition.
* Image Processing Libraries: OpenCV is essential for efficient image processing and real-time hand gesture recognition.
* Version Control: Implementation of Git for collaborative development and effective code management.
* IDE: Use of VSCode as the Integrated Development Environment for coding, debugging, and version control integration.
* Additional Dependencies: Includes scikit-learn, TensorFlow (versions 2.4.1), TensorFlow GPU, OpenCV, and Mediapipe for deep learning tasks.

## System Architecture

![img1](https://github.com/siva-169/Projectwork2/assets/113030675/1d57b2a7-dd3c-4eb4-8124-6bfc43d802cd)




## Output
#### Output1 - Name of the output

![WhatsApp Image 2024-04-04 at 8 53 43 AM (1)](https://github.com/siva-169/Projectwork2/assets/113030675/3fd3a15a-7d72-4be2-b189-802d3359857f)
#### Output2 - Name of the output
![WhatsApp Image 2024-04-04 at 8 53 43 AM](https://github.com/siva-169/Projectwork2/assets/113030675/76843758-3c73-4733-b885-a8e38d4a5894)
Detection Accuracy: 96.7%
Note: These metrics can be customized based on your actual performance evaluations.


## Results and Impact

The CNN algorithm was tested with to produce results. Every epoch observed a reduction in the loss throughout the training and test sets. Throughout all of the experiments, the batch size stayed consistent at 256. To acquire satisfactory results, the neural network design received the following changes: 
● Number of epochs: A positive association was established between the quantity of epochs and the precision of the model. But over fitting arose when there were too many epochs. Conclusion: Minimum overfitting and good accuracy were obtained using eight epochs. 
● Number of layers: With three hidden layers and one fully linked layer, the neural network architecture is formed. The activation function "relu" was utilised to generate a total of six convolution layers. 
● Filters: Based on the amount of filters applied to the picture, the neural network's accuracy on the dataset varies. First two network levels featured 64 filters each, while the third layer of the network retained 128 filters. 
● Accuracy: As the table illustrates, the ultimate, cutting-edge model produced training accuracy of 79.89% and test accuracy of 60.12%. Out of 28709 images in the train set and 2158 out of 3589 images in the test set, the architecture in use was able to classify images correctly. 


## Articles published / References
❏ H.-D. Nguyen, S. Yeom, G.-S. Lee, H.-J. Yang, I. Na, and S. H. Kim, "Facial Emotion Recognition Using an Ensemble of MultiLevel Convolutional Neural Networks," International Journal of Pattern Recognition and Artificial Intelligence, 2018.

❏ T. Cao and M. Li, "Facial Expression Recognition Algorithm Based on the Combination of CNN and K-Means," presented at the Proceedings of the 2019 11th International Conference on Machine Learning and Computing, Zhuhai, China, 2019. 

❏ N. Christou and N. Kanojiya, "Human Facial Expression Recognition with Convolutional Neural Networks," Singapore, 2019, pp. 539-545: Springer Singapore 

❏ A. Sajjanhar, Z. Wu, and Q. Wen, "Deep learning models for facial expression recognition," in 2018 Digital Image Computing: Techniques and Applications (DICTA), 2018, pp. 1-6: I EEE. 

❏ J. Chen, Y. Lv, R. Xu, and C. Xu, "Automatic social signal analysis: Facial expression recognition using difference convolution neural network," Journal of Parallel and Distributed Computing, vol. 131, pp. 97-102, 2019. 

❏ Al-Sumaidaee, Saadoon AM, et al, Multi-gradient features and elongated quinary pattern encoding for image-based facial expression recognition, Pattern Recognition, 2017, pp. 249—263. 
❏ Barsoum, Emad, et al, Training deep networks for facial expression recognition with crowd-sourced label distribution, ACM International Conference on Multimodal Interaction ACM, 2016, pp. 279—283.

❏ Martinez, Brais, et al, Automatic analysis of facial actions: A survey, IEEE Transactions on Affective Computing, 2017. 




